---
title: "Capstone_toxicity"
author: "Teena"
date: '2018-02-19'
output:
  pdf_document: default
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# install packages
```{r}
#install.packages("data.table")
#install.packages("tm")
#update.packages("tm",  checkBuilt = TRUE)
#install.packages("SnowballC")
#install.packages("rsconnect")
#install.packages("dplyr")
#install.packages("tidytext")
#install.packages("mldr")
#install.packages("Hmisc")
#install.packages("ggplot2")
library(ggplot2)
library(Hmisc)
library(mldr)
library("data.table")
library("tm")
library(SnowballC)
library(rsconnect)
library(dplyr)
library(tidytext)
```

importing of training data
```{r}
toxicdata = fread("train.csv", header = "auto", sep="auto", nrows=-1L,blank.lines.skip=TRUE, encoding="UTF-8")
```
exploratory analysis - toxic, severe_toxic, obscene, threat, insult, identity_hate / toxic, obscene, insult are thee classes that go together
```{r}
summary(toxicdata)
str(toxicdata)
dim(toxicdata)
colSums(sapply(toxicdata, is.na))
toxicdata2<-toxicdata[,c(3:8)]
# plotting number of documents with each toxicity category
x<-barplot(colSums(toxicdata2), ylim = c(0,20000), xlab="type of toxicity", ylab="frequency", main ="Frequency of each toxicity level")
y<-as.matrix(colSums(toxicdata2))
text(x, y, labels=as.character(y), pos = 3, cex=1)
toxicdata$toxcount<-rowSums(toxicdata[,3:8])
ggsave("Freq of Doc by toxic labels.png", width=297, height =210, units = "mm")
# plotting number of documents with the number of labels per document
Num_Class<-table(toxicdata$toxcount)
tox_class<-as.data.frame(Num_Class, row.names = NULL, responseName = "Num_Doc", sep=" ")
c1<-ggplot(data = subset(tox_class, Var1!=0), aes(x=Var1, y=Num_Doc)) + geom_bar(stat="identity")+ggtitle("Document frequency for multi labels")+geom_text(aes(label = Num_Doc), vjust = 1.5, color = "red")
ggsave("Document frequency for multi labels.png", width=297, height =210, units = "mm")
c1

avg_label<-sum(toxicdata$toxcount)/nrow(toxicdata) #Avg labels per document
avg_label_notoxic<-sum(toxicdata$toxcount)/nrow(toxicdata[which(toxicdata$toxcount>0)]) #Avg label per doc with no toxic comments

```

frequency of unique words
```{r}
commentrev <- toxicdata$comment_text
sentences<-gsub("\\.","",commentrev)
sentences1<-gsub("\\,","",sentences)
words1 <- tolower(sentences1)
words2 <- strsplit(words1, "\\W")
words3 <- unlist(words2)
freq <- table(words3)
freq1 <- sort(freq, decreasing = TRUE)
freq1_table <-as.data.frame(freq1, row.names=NULL, responseName="Freq", sep= " ")
test<-subset(freq1_table, words3!="")

stopWords <- c(stopwords("en"), "can", "will", "don", "now", "just", "also", "may", "get", "well", "need", "say", "way", "want", "see", "read", "look", "stop", "like", "really", "however", "let", "ask", "used", "made", "much", "utc", "added", "didn", "sure", "put", "better", "using", "tell", "anything", "one", "two")
class(stopWords)

test2 <- subset(test,words3 %nin% stopWords)
test2$len <- nchar(as.character(test2$words3))

test3 <-subset(test2, len>2)
test4<-test3[1:100,1:2]
p1<-ggplot(data = test4, aes(x=words3, y=Freq)) + geom_bar(stat="identity")+coord_flip()+ggtitle("Top 100 unique words across all documents")
ggsave("Top 100 unique words across all documents.png", width=297, height =210, units = "mm")

nrow(test) #number of unique words in all documents
head(test3) # top 5 words under all docs
tail(test3) # lowest 5 words under all docs

#p + geom_col() or p + geom_bar(aes(weight=Freq))
#temple.sorted.table <- paste(names(freq1), freq1, sep="\t")
#cat("Word\tFreQ", temple.sorted.table, file="mywordfreq.txt", sep="\n")

```
words for label toxic 
```{r}

toxicdata_to<-subset(toxicdata, toxic==1)
commentrev <- toxicdata_to$comment_text
sentences<-gsub("\\.","",commentrev)
sentences1<-gsub("\\,","",sentences)
words1 <- tolower(sentences1)
words2 <- strsplit(words1, "\\W")
words3 <- unlist(words2)
freq <- table(words3)
freq1 <- sort(freq, decreasing = TRUE)
freq1_table <-as.data.frame(freq1, row.names=NULL, responseName="Freq", sep= " ")
test<-subset(freq1_table, words3!="")

stopWords <- c(stopwords("en"), "can", "will", "don", "now", "just", "also", "may", "get", "well", "need", "say", "way", "want", "see", "read", "look", "stop", "like", "really", "however", "let", "ask", "used", "made", "much", "utc", "added", "didn", "sure", "put", "better", "using", "tell", "anything", "one", "two")
class(stopWords)

test2 <- subset(test,words3 %nin% stopWords)
test2$len <- nchar(as.character(test2$words3))

test3 <-subset(test2, len>2)
test4<-test3[1:100,1:2]
p2<-ggplot(data = test4, aes(x=words3, y=Freq)) + geom_bar(stat="identity")+coord_flip()+ggtitle("Top 100 unique words in 'Toxic' label")
ggsave("Top 100 unique words in Toxic label.png", width=297, height =210, units = "mm")
nrow(test) #number of unique words under toxic label
head(test3) # top 5 words under toxic label
tail(test3) # lowest 5 words under toxic label
```

words for label severe 
```{r}
toxicdata_sto<-subset(toxicdata, severe_toxic==1)
commentrev <- toxicdata_sto$comment_text
sentences<-gsub("\\.","",commentrev)
sentences1<-gsub("\\,","",sentences)
words1 <- tolower(sentences1)
words2 <- strsplit(words1, "\\W")
words3 <- unlist(words2)
freq <- table(words3)
freq1 <- sort(freq, decreasing = TRUE)
freq1_table <-as.data.frame(freq1, row.names=NULL, responseName="Freq", sep= " ")
test<-subset(freq1_table, words3!="")

stopWords <- c(stopwords("en"), "can", "will", "don", "now", "just", "also", "may", "get", "well", "need", "say", "way", "want", "see", "read", "look", "stop", "like", "really", "however", "let", "ask", "used", "made", "much", "utc", "added", "didn", "sure", "put", "better", "using", "tell", "anything", "one", "two")
class(stopWords)

test2 <- subset(test,words3 %nin% stopWords)
test2$len <- nchar(as.character(test2$words3))

test3 <-subset(test2, len>2)
test4<-test3[1:100,1:2]
p3<-ggplot(data = test4, aes(x=words3, y=Freq)) + geom_bar(stat="identity")+coord_flip()+ggtitle("Top 100 unique words in 'Severe' label")
ggsave("Top 100 unique words in Severe Toxic label.png", width=297, height =210, units = "mm")
nrow(test) #number of unique words under severe toxic label
head(test3) # top 5 words under severe toxic label
tail(test3) # lowest 5 words under severe toxic label
```

words for label obscene 
```{r}
toxicdata_o<-subset(toxicdata, obscene==1)
commentrev <- toxicdata_o$comment_text
sentences<-gsub("\\.","",commentrev)
sentences1<-gsub("\\,","",sentences)
words1 <- tolower(sentences1)
words2 <- strsplit(words1, "\\W")
words3 <- unlist(words2)
freq <- table(words3)
freq1 <- sort(freq, decreasing = TRUE)
freq1_table <-as.data.frame(freq1, row.names=NULL, responseName="Freq", sep= " ")
test<-subset(freq1_table, words3!="")

stopWords <- c(stopwords("en"), "can", "will", "don", "now", "just", "also", "may", "get", "well", "need", "say", "way", "want", "see", "read", "look", "stop", "like", "really", "however", "let", "ask", "used", "made", "much", "utc", "added", "didn", "sure", "put", "better", "using", "tell", "anything", "one", "two")
class(stopWords)

test2 <- subset(test,words3 %nin% stopWords)
test2$len <- nchar(as.character(test2$words3))

test3 <-subset(test2, len>2)
test4<-test3[1:100,1:2]
p4<-ggplot(data = test4, aes(x=words3, y=Freq)) + geom_bar(stat="identity")+coord_flip()+ggtitle("Top 100 unique words in 'Obscene' label")
ggsave("Top 100 unique words in Obscene label.png", width=297, height =210, units = "mm")
nrow(test) #number of unique words under obscene toxic label
head(test3) # top 5 words under obscene toxic label
tail(test3) # lowest 5 words under obscene toxic label

```

words for label threat 
```{r}
toxicdata_t<-subset(toxicdata, threat==1)
commentrev <- toxicdata_t$comment_text
sentences<-gsub("\\.","",commentrev)
sentences1<-gsub("\\,","",sentences)
words1 <- tolower(sentences1)
words2 <- strsplit(words1, "\\W")
words3 <- unlist(words2)
freq <- table(words3)
freq1 <- sort(freq, decreasing = TRUE)
freq1_table <-as.data.frame(freq1, row.names=NULL, responseName="Freq", sep= " ")
test<-subset(freq1_table, words3!="")

stopWords <- c(stopwords("en"), "can", "will", "don", "now", "just", "also", "may", "get", "well", "need", "say", "way", "want", "see", "read", "look", "stop", "like", "really", "however", "let", "ask", "used", "made", "much", "utc", "added", "didn", "sure", "put", "better", "using", "tell", "anything", "one", "two")
class(stopWords)

test2 <- subset(test,words3 %nin% stopWords)
test2$len <- nchar(as.character(test2$words3))

test3 <-subset(test2, len>2)
test4<-test3[1:100,1:2]
p5<-ggplot(data = test4, aes(x=words3, y=Freq)) + geom_bar(stat="identity")+coord_flip()+ggtitle("Top 100 unique words in 'Threat' label")
ggsave("Top 100 unique words in Threat label.png", width=297, height =210, units = "mm")
nrow(test) #number of unique words under threat toxic label
head(test3) # top 5 words under threat toxic label
tail(test3) # lowest 5 words under threat toxic label
```

words for label insult 
```{r}
toxicdata_in<-subset(toxicdata, insult==1)
commentrev <- toxicdata_in$comment_text
sentences<-gsub("\\.","",commentrev)
sentences1<-gsub("\\,","",sentences)
words1 <- tolower(sentences1)
words2 <- strsplit(words1, "\\W")
words3 <- unlist(words2)
freq <- table(words3)
freq1 <- sort(freq, decreasing = TRUE)
freq1_table <-as.data.frame(freq1, row.names=NULL, responseName="Freq", sep= " ")
test<-subset(freq1_table, words3!="")

stopWords <- c(stopwords("en"), "can", "will", "don", "now", "just", "also", "may", "get", "well", "need", "say", "way", "want", "see", "read", "look", "stop", "like", "really", "however", "let", "ask", "used", "made", "much", "utc", "added", "didn", "sure", "put", "better", "using", "tell", "anything", "one", "two")
class(stopWords)

test2 <- subset(test,words3 %nin% stopWords)
test2$len <- nchar(as.character(test2$words3))

test3 <-subset(test2, len>2)
test4<-test3[1:100,1:2]
p6<-ggplot(data = test4, aes(x=words3, y=Freq)) + geom_bar(stat="identity")+coord_flip()+ggtitle("Top 100 unique words in 'Insult' label")
ggsave("Top 100 unique words in Insult label.png", width=297, height =210, units = "mm")
nrow(test) #number of unique words under Insult toxic label
head(test3) # top 5 words under Insult toxic label
tail(test3) # lowest 5 words under Insult toxic label
```

words for label identity_hate 
```{r}
toxicdata_ih<-subset(toxicdata, identity_hate==1)
commentrev <- toxicdata_ih$comment_text
sentences<-gsub("\\.","",commentrev)
sentences1<-gsub("\\,","",sentences)
words1 <- tolower(sentences1)
words2 <- strsplit(words1, "\\W")
words3 <- unlist(words2)
freq <- table(words3)
freq1 <- sort(freq, decreasing = TRUE)
freq1_table <-as.data.frame(freq1, row.names=NULL, responseName="Freq", sep= " ")
test<-subset(freq1_table, words3!="")

stopWords <- c(stopwords("en"), "can", "will", "don", "now", "just", "also", "may", "get", "well", "need", "say", "way", "want", "see", "read", "look", "stop", "like", "really", "however", "let", "ask", "used", "made", "much", "utc", "added", "didn", "sure", "put", "better", "using", "tell", "anything", "one", "two")
class(stopWords)

test2 <- subset(test,words3 %nin% stopWords)
test2$len <- nchar(as.character(test2$words3))

test3 <-subset(test2, len>2)
test4<-test3[1:100,1:2]
p7<-ggplot(data = test4, aes(x=words3, y=Freq)) + geom_bar(stat="identity")+coord_flip()+ggtitle("Top 100 unique words in 'identity_hate' label")
ggsave("Top 100 unique words in identity_hate label.png", width=297, height =210, units = "mm")
nrow(test) #number of unique words under identity_hate toxic label
head(test3) # top 5 words under identity_hate toxic label
tail(test3) # lowest 5 words under identity_hate toxic label
```

words for label toxic, obscene, insult 
```{r}
toxicdata_3label<-subset(toxicdata, toxic==1 & obscene==1 & insult==1)
commentrev <- toxicdata_3label$comment_text
sentences<-gsub("\\.","",commentrev)
sentences1<-gsub("\\,","",sentences)
words1 <- tolower(sentences1)
words2 <- strsplit(words1, "\\W")
words3 <- unlist(words2)
freq <- table(words3)
freq1 <- sort(freq, decreasing = TRUE)
freq1_table <-as.data.frame(freq1, row.names=NULL, responseName="Freq", sep= " ")
test<-subset(freq1_table, words3!="")

stopWords <- c(stopwords("en"), "can", "will", "don", "now", "just", "also", "may", "get", "well", "need", "say", "way", "want", "see", "read", "look", "stop", "like", "really", "however", "let", "ask", "used", "made", "much", "utc", "added", "didn", "sure", "put", "better", "using", "tell", "anything", "one", "two")
class(stopWords)

test2 <- subset(test,words3 %nin% stopWords)
test2$len <- nchar(as.character(test2$words3))

test3 <-subset(test2, len>2)
test4<-test3[1:100,1:2]
label3<-ggplot(data = test4, aes(x=words3, y=Freq)) + geom_bar(stat="identity")+coord_flip()+ggtitle("Top 100 unique words in '3 labels - toxic, obscene and insult' label")
ggsave("Top 100 unique words overlapping over 3 labels - toxic, obscene and insult label.png", width=297, height =210, units = "mm")
nrow(test) #number of unique words under 3 labels - toxic, obscene and insult toxic label
head(test3) # top 5 words under 3 labels - toxic, obscene and insult toxic label
tail(test3) # lowest 5 words under identity_hate toxic label
```

steps for text classification in R
```{r}
library(tm)
library(plyr)
library(dplyr)
library(class)

# step 1 - corpus creation (note need to select the field and it has to be in vector format)
cor <- Corpus(VectorSource(toxicdata$comment_text))

# step 2 - preprocessing of corpus - remove punctuations, white spaces, lower case and stop words

corpus <- tm_map(cor, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removeWords, stopwords("english"))

# step 3 - term document matrix

tdm <-TermDocumentMatrix(corpus)

# step 4: Feature Extraction & Labels for the model

tdm_sparse <- removeSparseTerms(tdm,0.90)
tdm
tdm_sparse

tdm_dm <- as.data.frame(as.matrix(tdm_sparse)) #count matrix

tdm_df <- as.matrix((tdm_dm>0)+0) #binary instance matrix

tdm_df <- as.data.frame(tdm_df)

#tdm_df_append <- cbind(tdm_df, toxicdata$id)

```

